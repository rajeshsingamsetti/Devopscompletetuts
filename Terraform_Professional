//Professional terraform tutorial
***** aws terraform documentation:
https://registry.terraform.io/providers/hashicorp/aws/latest/docs


majorly we use in terraform 3 files
1. main.tf or vpc.tf
2.variable.tf
3.terra.tfvars

reference github url:
https://github.com/mavrick202/terraformsingleinstance

above code main.tf file is
===========================================================
#This Terraform Code Deploys Basic VPC Infra.
provider "aws" {
    access_key = "${var.aws_access_key}"
    secret_key = "${var.aws_secret_key}"
    region = "${var.aws_region}"
}

resource "aws_vpc" "default" {
    cidr_block = "${var.vpc_cidr}"
    enable_dns_hostnames = true
    tags = {
        Name = "${var.vpc_name}"
	Owner = "Sreeharsha Veerapalli"
	environment = "${var.environment}"
    }
}

resource "aws_internet_gateway" "default" {
    vpc_id = "${aws_vpc.default.id}"
	tags = {
        Name = "${var.IGW_name}"
    }
}

resource "aws_subnet" "subnet1-public" {
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${var.public_subnet1_cidr}"
    availability_zone = "us-east-1a"

    tags = {
        Name = "${var.public_subnet1_name}"
    }
}

resource "aws_subnet" "subnet2-public" {
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${var.public_subnet2_cidr}"
    availability_zone = "us-east-1b"

    tags = {
        Name = "${var.public_subnet2_name}"
    }
}

resource "aws_subnet" "subnet3-public" {
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${var.public_subnet3_cidr}"
    availability_zone = "us-east-1c"

    tags = {
        Name = "${var.public_subnet3_name}"
    }
	
}


resource "aws_route_table" "terraform-public" {
    vpc_id = "${aws_vpc.default.id}"

    route {
        cidr_block = "0.0.0.0/0"
        gateway_id = "${aws_internet_gateway.default.id}"
    }

    tags = {
        Name = "${var.Main_Routing_Table}"
    }
}

resource "aws_route_table_association" "terraform-public" {
    subnet_id = "${aws_subnet.subnet1-public.id}"
    route_table_id = "${aws_route_table.terraform-public.id}"
}

resource "aws_security_group" "allow_all" {
  name        = "allow_all"
  description = "Allow all inbound traffic"
  vpc_id      = "${aws_vpc.default.id}"

  ingress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port       = 0
    to_port         = 0
    protocol        = "-1"
    cidr_blocks     = ["0.0.0.0/0"]
    }
}

# data "aws_ami" "my_ami" {
#      most_recent      = true
#      #name_regex       = "^mavrick"
#      owners           = ["721834156908"]
# }


# resource "aws_instance" "web-1" {
#     ami = var.imagename
#     #ami = "ami-0d857ff0f5fc4e03b"
#     #ami = "${data.aws_ami.my_ami.id}"
#     availability_zone = "us-east-1a"
#     instance_type = "t2.micro"
#     key_name = "LaptopKey"
#     subnet_id = "${aws_subnet.subnet1-public.id}"
#     vpc_security_group_ids = ["${aws_security_group.allow_all.id}"]
#     associate_public_ip_address = true	
#     tags = {
#         Name = "Server-1"
#         Env = "Prod"
#         Owner = "Sree"
# 	CostCenter = "ABCD"
#     }
# }

##output "ami_id" {
#  value = "${data.aws_ami.my_ami.id}"
#}
#!/bin/bash
# echo "Listing the files in the repo."
# ls -al
# echo "+++++++++++++++++++++++++++++++++++++++++++++++++++++"
# echo "Running Packer Now...!!"
# packer build -var=aws_access_key=AAAAAAAAAAAAAAAAAA -var=aws_secret_key=BBBBBBBBBBBBB packer.json
#packer validate --var-file creds.json packer.json
#packer build --var-file creds.json packer.json
#packer.exe build --var-file creds.json -var=aws_access_key=AAAAAAAAAAAAAAAAAA -var=aws_secret_key=BBBBBBBBBBBBB packer.json
# echo "+++++++++++++++++++++++++++++++++++++++++++++++++++++"
# echo "Running Terraform Now...!!"
# terraform init
# terraform apply --var-file terraform.tfvars -var="aws_access_key=AAAAAAAAAAAAAAAAAA" -var="aws_secret_key=BBBBBBBBBBBBB" --auto-approve
#https://discuss.devopscube.com/t/how-to-get-the-ami-id-after-a-packer-build/36

==================================================

above code open vscode editor
expect main.tf , output.tfx, terraform.tvars,variable.tf remove all the files

the diff b/w var.tf and terraform.tfvars (is we can change the dynamic code only in .tfvars insted of changing the total code

the procedure is
main.tf ==> variable.tf => terra.tfvars (we don't change the main code we only change in tfvars)
======================================terraform real time project udemy================
https://github.com/Sanjeev-Thiyagarajan/Terraform-Crash-Course
=============================================================================
from now again madhu


terraform init
terraform plan -var_file terraform.tfvars --auto-approve
** terraform most useful commands
Common commands:
    apply              Builds or changes infrastructure
    console            Interactive console for Terraform interpolations
    destroy            Destroy Terraform-managed infrastructure
    env                Workspace management
    fmt                Rewrites config files to canonical format
    get                Download and install modules for the configuration
    graph              Create a visual graph of Terraform resources
    import             Import existing infrastructure into Terraform
    init               Initialize a Terraform working directory
    output             Read an output from a state file
    plan               Generate and show an execution plan
    providers          Prints a tree of the providers used in the configuration
    refresh            Update local state file against real resources
    show               Inspect Terraform state or plan
    taint              Manually mark a resource for recreation
    untaint            Manually unmark a resource as tainted
    validate           Validates the Terraform files
    version            Prints the Terraform version
    workspace          Workspace management

All other commands:
    0.12upgrade        Rewrites pre-0.12 module source code for v0.12
    debug              Debug output management (experimental)
    force-unlock       Manually unlock the terraform state
    push               Obsolete command for Terraform Enterprise legacy (v1)
    state              Advanced state management
===================

terraform init
terraform plan --var-file prod.tfvars
terraform apply --auto-approve
terraform destroy

aws s3 bucket creation 
refurl: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket


terraform is an idempodent(once we made changes and apply again you run it it won't affect bcoz it will monitor the state file)

** must be take care of state file

                                                                              types of files
.tf
.tfvars
.state

real time

clone github repo
https://github.com/mavrick202/terraformsingleinstance

rm -rf docker.service jenkibuildscript packer.json 

after
the main code will be there in main.tf file

flow
main to variable.tf --> .tfvars
in output file remove all the existing data and add the below code to get vpc name

output "vpc_name" {
  description = "list of vpc name"
  value = ["${aws_vpc.default.tags.Name}"]
}


"change terraform.tfvars into prod.tfvars"

in that file give ur aws acceesskey & secreat key


*** now we have to run our dry run
terraform plan --var-file prod.tfvars

terraform apply --var-file prod.tfvars (it will apply on real time on to our servers)

** to know what we are deployed we can known bt
terraform state list

* terraform show (it will clearly explain the what we did in our operation)


** Note one folder inside we can't run multiple tfvar files bcoz state problem(it will overwrite the old data)


** to destory
terraform destory --var-file prod.tfvars



Note : if u want to run multipletfvars (when we apply and running time it will delete the previous configurations 
**** single folder we can't run multiple tfvars
for this we need seperate workspace like ex: prodfolder , testfolder (for its safe side)

insteed we can use workspace now we are going to remove the 
terraform.tfstate
terraform.tfstate.backup

copy above files at somewhere

** now when we work on multiple tfvars we can use "workspace" its nothing but a git branch type

terraform workspace show (it displays the workspaces of terraform)

to create a new work space
terraform workspace new prod

terraform workspace list

terraform workspace new dev
after it will create an separate folder for us in the name of dev

* to switch workspace
terrraform workspace select dev

* to delete workspace
first u need to switch the another folder
after delete the folder what you want to delete
terraform workspace delete foldername

** after creating workspace go to that folder
and
terraform apply --var-file prod.tfvars --auto-approve
after it will copy only the tfstate to that specific folder

****note u must be there correct workspace(to avoid overites in production)

this procedure for single folder run multiple tfvar files

terraform plan --var-file dev.tfvars (by using workspace we can use muktiple variable files)
terraform plan --var-file prod.tfvars (by using workspace we can use muktiple variable files)

Note "
never user workspace bcoz of thread(overwritten the all files) insted of we can use multiple folders

*** for see the outputs 
terraform output

**note: after destroy only we need to delete workspace

*** we never share or push a state file to github

 now we copy our entire project and pasted in another location

now parent folder apply the terraform
now go to same child (note terraform.tfstate file must be there ) otherwise its duplicates your data

*** state file centralization we never encourage statefile will need to move on github
            ------------------- aws terraform backend-----------------------------------------
""" insted of coping everytime the .tfstate file we can use terraform state backend"""

we can use terraform back end
create bucket in s3 enable versioning 
go to aws create aws s3 bucket

terraform {
  backend "s3" {
    bucket = "terrabackendraj"
    key    = "myterraform.tfstate"
    region = "us-east-1"
  }
}

ref url: https://github.com/mavrick202/terraformsingleinstance/blob/remotebackends3/main.tf

add s3 code into main.tf or else create separe file like
backend.tf (paste the below code)

terraform {
  backend "s3" {
    bucket = "terrabucketn1"
    key    = "myterraform.tfstate"
    region = "us-east-1"
  }
}




*** reference main.rf
#This Terraform Code Deploys Basic VPC Infra.
provider "aws" {
    access_key = "${var.aws_access_key}"
    secret_key = "${var.aws_secret_key}"
    region = "${var.aws_region}"
}

resource "aws_vpc" "default" {
    cidr_block = "${var.vpc_cidr}"
    enable_dns_hostnames = true
    tags = {
        Name = "${var.vpc_name}"
	Owner = "Sreeharsha Veerapalli"
	environment = "${var.environment}"
    }
}

resource "aws_internet_gateway" "default" {
    vpc_id = "${aws_vpc.default.id}"
	tags = {
        Name = "${var.IGW_name}"
    }
}

resource "aws_subnet" "subnet1-public" {
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${var.public_subnet1_cidr}"
    availability_zone = "us-east-1a"

    tags = {
        Name = "${var.public_subnet1_name}"
    }
}

resource "aws_subnet" "subnet2-public" {
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${var.public_subnet2_cidr}"
    availability_zone = "us-east-1b"

    tags = {
        Name = "${var.public_subnet2_name}"
    }
}

resource "aws_subnet" "subnet3-public" {
    vpc_id = "${aws_vpc.default.id}"
    cidr_block = "${var.public_subnet3_cidr}"
    availability_zone = "us-east-1c"

    tags = {
        Name = "${var.public_subnet3_name}"
    }
	
}


resource "aws_route_table" "terraform-public" {
    vpc_id = "${aws_vpc.default.id}"

    route {
        cidr_block = "0.0.0.0/0"
        gateway_id = "${aws_internet_gateway.default.id}"
    }

    tags = {
        Name = "${var.Main_Routing_Table}"
    }
}

resource "aws_route_table_association" "terraform-public" {
    subnet_id = "${aws_subnet.subnet1-public.id}"
    route_table_id = "${aws_route_table.terraform-public.id}"
}

resource "aws_security_group" "allow_all" {
  name        = "allow_all"
  description = "Allow all inbound traffic"
  vpc_id      = "${aws_vpc.default.id}"

  ingress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port       = 0
    to_port         = 0
    protocol        = "-1"
    cidr_blocks     = ["0.0.0.0/0"]
    }
}

terraform {
  backend "s3" {
    bucket = "terrabucketn1"
    key    = "myterraform.tfstate"
    region = "us-east-1"
  }
}


# data "aws_ami" "my_ami" {
#      most_recent      = true
#      #name_regex       = "^mavrick"
#      owners           = ["721834156908"]
# }


# resource "aws_instance" "web-1" {
#     ami = var.imagename
#     #ami = "ami-0d857ff0f5fc4e03b"
#     #ami = "${data.aws_ami.my_ami.id}"
#     availability_zone = "us-east-1a"
#     instance_type = "t2.micro"
#     key_name = "LaptopKey"
#     subnet_id = "${aws_subnet.subnet1-public.id}"
#     vpc_security_group_ids = ["${aws_security_group.allow_all.id}"]
#     associate_public_ip_address = true	
#     tags = {
#         Name = "Server-1"
#         Env = "Prod"
#         Owner = "Sree"
# 	CostCenter = "ABCD"
#     }
# }

##output "ami_id" {
#  value = "${data.aws_ami.my_ami.id}"
#}
#!/bin/bash
# echo "Listing the files in the repo."
# ls -al
# echo "+++++++++++++++++++++++++++++++++++++++++++++++++++++"
# echo "Running Packer Now...!!"
# packer build -var=aws_access_key=AAAAAAAAAAAAAAAAAA -var=aws_secret_key=BBBBBBBBBBBBB packer.json
#packer validate --var-file creds.json packer.json
#packer build --var-file creds.json packer.json
#packer.exe build --var-file creds.json -var=aws_access_key=AAAAAAAAAAAAAAAAAA -var=aws_secret_key=BBBBBBBBBBBBB packer.json
# echo "+++++++++++++++++++++++++++++++++++++++++++++++++++++"
# echo "Running Terraform Now...!!"
# terraform init
# terraform apply --var-file terraform.tfvars -var="aws_access_key=AAAAAAAAAAAAAAAAAA" -var="aws_secret_key=BBBBBBBBBBBBB" --auto-approve
#https://discuss.devopscube.com/t/how-to-get-the-ami-id-after-a-packer-build/36


============================ Terraform Dynamodblocking==================
When multiple enginers working on same project same files at that time we may face some problems due to conflicts on working on same files to avoid that cause create 
locking system

as of now assume i had configured terraform same project on my mac and aws ubuntu machine at that time we work on same project same file we may get conflict to overcome
we can use the locking system

open aws and launch ubuntu machine

create one private repo on github
create one file i ur terraform project as the 
.gitignore
.terraform
*.exe
efs.yml

rm -rf .git
git init
git remote add origin add private url
git status (now u will observer .terraform etc file will removed)

git add .
git commit -m "terra main code"
git push origin master

after pushing our code is: https://github.com/rajeshsingamsetti/terraformprivate/tree/master

Now go to ubuntu machine launch it

ssh-keygen
hit enter 3 times

cat /root/.ssh/id_rsa.pub

copy the total code and go to git that particular private repo and settings

left side click on deploy keys under ssh paste your copy code give any title u want 

come to ubuntu machine clone ur private repo
choose clone ssh ur github private repo (not http go with ssh)
git clone sshurl

for ubuntu create a new iam role for terraform

now we need to install aws cli on ubuntu machine

for ubuntu 18.03 install aws cli through

apt-get update -y
apt-get update install awscli
aws --version

go to aws create a user and add take acees key
aws configure

now go to our mac now according to code
create a bucket in s3 aws (in the name of our terraform code)
in mac
terraform init
terraform plan --var-file prod.tfvars (or terraform plan -var-file terraform.tfvars)
terraform apply

if any changes in mac commite it
git status

now come to ubuntu machine
cd project file
git pull sshurl master

terraform state list

Command 'terraform' not found, but can be installed with

terraform not found on ubuntu we need to install terraform on ubuntu
install on ubuntu
cd /usr/local/bin/
wget https://releases.hashicorp.com/terraform/0.13.5/terraform_0.13.5_linux_amd64.zip
unzip terraform_0.13.5_linux_amd64.zip 

cd terraformprivate/(our main git clone project on root)

terraform init
terraform state list

remove output.tfx and make it output.tf and paste the below code
ouput "vpc_name" {
  description = "list of vpc name"
  value = ["${aws_vpc.default.id}"]
}

after terraform init
terraform apply

git status (now output file will be changed)
git add . && git commit -m "changes"
git push origin master

go to github now the code is changed

Now come to mac user 
git pull https://github.com/rajeshsingamsetti/terraformprivate.git master(url of github master)

now we had chances that multiple people override for that we are using lock system


use the below code create a onefile in mac name it as a dynamodb.tf

# example.tf
# create a dynamodb table for locking the state file
resource "aws_dynamodb_table" "dynamodb-terraform-state-lock" {
  name = "terraform-state-lock-dynamo"
  hash_key = "LockID"
  read_capacity = 20
  write_capacity = 20
 
  attribute {
    name = "LockID"
    type = "S"
  }
 
  tags = {
    Name = "DynamoDB Terraform State Lock Table"
  }
}

now run terraform plan (it will create dynamodb table we can see that in terminal)
after plan success apply it
terraform apply

after go to aws dynamodb 
we will see on table in under dynamodb

after creating dynamo db table now you need to add 2 lines od code in your s3backend as it shown below like this

encrypt = true
dynamodb_table = "terraform-state-lock-dynamo"
    
now under main.tf under s3 file shold be like this

terraform {
  backend "s3" {
    bucket = "terrabucketn1"
    key    = "myterraform.tfstate"
    region = "us-east-1"
    encrypt = true
    dynamodb_table = "terraform-state-lock-dynamo"

  }
 }
 
 

after 
terraform apply (it will throgh error bcoz we need to intialize)
after intialize successfully change ur code small correction like add tags then apply it)
terraform apply
now go to aws dynamo db u will see one table means (dynamolock configure successfully)

now in ur mac user
git status
git add .
git commit -m  "dynamo lock"
git push origin master

Now go to ubunti user 

git pull git@github.com:rajeshsingamsetti/terraformprivate.git master


***** now go to mac user change the name or anything and save it

terraform apply (note now don't type yes)

now go to ubuntu server

terraform apply (you will see init if error comes init it after apply it now u can see that its locked)

*****now we will see the lock like this****
Error: Error locking state: Error acquiring the state lock: ConditionalCheckFailedException: The conditional request failed
Lock Info:
  ID:        57a7fa51-35e5-9969-d51f-ae6e6e897010
  Path:      terrabucketn1/myterraform.tfstate
  Operation: OperationTypeApply
  Who:       rajsingam@RAJs-MBP
  Version:   0.13.5
  Created:   2020-11-29 16:28:06.345226 +0000 UTC
  Info:      


Terraform acquires a state lock to protect the state from being written
by multiple users at the same time. Please resolve the issue above and try
again. For most commands, you can disable locking with the "-lock=false"
flag, but this is not recommended.
*******************************************************************
after yes
git add . && git commit -m "mac userdata"
git push origin master


now in ubutu 


git pull git@github.com:rajeshsingamsetti/terraformprivate.git master

change name or any thing 
terraform apply
don't type yes

now come to mac user type 
terraform apply

now we will get lock error on mac user also

** so now we will get lock in all user by using multiple admin we will work same time without any issues

in real time never give 
-lock=false
terraform apply -lock=false(it will cause issues)

===============================terraform data points=======================================================
terraform import => imports manually created resources in to configuration and we can manage it even delete it
terraform DataSources => let terraform know about the resources which are the not part of it
"to manage the existing resources but don't had an right of delete it"

"terraform refresh is used for refreshing of the state of terraform"

add this below code in ur main.tf file
it will fetch the exsiting vpc details

data "aws_vpc" "Rajvpc" {
  id = "vpc-0d143ca90e92391ad"
}
now run 
terraform refresh

***** note: once by using datasource vpc or anything is created its related all attributes are imported from that particular service like tagname,cidr_block,routetable_name.

by using data source we create vpc and one subnet

data "aws_vpc" "Rajvpc" {
  id = "vpc-0d143ca90e92391ad"
}

resource "aws_subnet" "rajvpcsubnet4-public" {
    vpc_id = "${data.aws_vpc.Rajvpc.id}"
    cidr_block = "10.0.4.0/24"
    availability_zone = "us-east-1a"

    tags = {
        Name = "Rajvpc-subnet-4"
    }
}

add above code in ur main.tf file

====================== to delete a specific resource from terraform========
terraform destroy -target aws_subnet.rajvpcsubnet4-public

to remove from state list (bcoz if u delete particulat subnet code also it exist on state to remove from state list also follow below procedure)
terraform state list

to remove from state list
terraform state rm aws_subnet.rajvpcsubnet4-public

===========================================================================================================================================================
					     Functions
without code duplication create subnets in different availability zones and diff cidr's

az = us-east-1a,us-east-1b,us-east-1c
cidrs = 10.1.1.0/24,10.1.2.0/24,10.1.3.0/24,10.1.4.0/24,10.1.5.0/24

to avoid code duplicate (pasting subnets multiple times) we can use terraform functions

to set variables on variables.tf file
variable "azs" {
  description = "Run the EC2 Instances in these Availability Zones"
  type = "list"
  default = ["us-east-1a", "us-east-1b", "us-east-1c"]
}
variable "cidrs" {
  description = "Cidr's for subnets"
  type = "list"
  default = ["10.1.1.0/24", "10.1.2.0/24", "10.1.3.0/24"]
}


main.tf 

ref url of functions:
https://www.terraform.io/docs/configuration/functions.html

in functions we use
length
count
element


terraform functions reference github url:
https://github.com/rajeshsingamsetti/terraformfunctions1/tree/master


=============================================terraform another functions======
lookups : ref url: https://github.com/rajeshsingamsetti/Devopscompletetuts/tree/main/Terraformsaple
conditional

null resource and taint usage
local-exec
remote exec
for_each
depends_on
locals
			lookup
without lookup if u run ur code in any another region it fails by using lookup your code will run in any regions 
https://github.com/rajeshsingamsetti/terraformfunctions1/tree/master

remove state files
and rename the dynamodb.tf to dynamo.tf1
and go to main.tf

comment the backend s3
# terraform {
#   backend "s3" {
#     bucket = "terrabucketn1"
#     key    = "myterraform.tfstate"
#     region = "us-east-1"
#     encrypt = true
#     dynamodb_table = "terraform-state-lock-dynamo"

#   }
# }

and ec2.tf the private subnet total bracket should be in comments we enough to public subnet

now by using lookups whereever the region will be there it will take automatically

ec2.tf
example: ami = "${lookup(var.amis, var.aws_region, "us-east-1")}"

in variable.tf be like below

variable "aws_region" {}
variable "amis" {
    description = "AMIs by region"
    default = {
      us-east-1 = "ami-00ddb0e5626798373" # ubuntu 14.04 LTS(us region ami number give here)
      us-east-2 = "ami-0ebc8f6f580a04647" # ubuntu 14.04 LTS
    }
}

variable "environment" {  }


in main.tf file remove below code private nat remove
//private route
resource "aws_route_table_association" "terraform-private" {
    count = "${length(var.private-cidrs)}"
    subnet_id = "${element(aws_subnet.private-subnets.*.id, count.index)}"
    route_table_id = "${aws_route_table.terraform-private.id}"
}

//private routing table
resource "aws_route_table" "terraform-private" {
    vpc_id = "${aws_vpc.default.id}"

    route {
        cidr_block = "0.0.0.0/0"
        gateway_id = "${aws_nat_gateway.ngw.id}"
    }

    tags = {
        Name = "${var.Private_Routing_Table}"
    }
}

and rename the eip-nat.tfx (elastic ip rename the file)

go to terraform.tfvars
change the acees key and security key
**note ** - keypair must be exist on our server region otherwise itwill through an error
*** ex:key_name = "east111" (keyname must exist on our server).
environment="Prod"

terraform init 
terraform plan

after sucess run instance created ami it will take the region of us-east1

Conditional Expressions

refurl: https://www.terraform.io/docs/configuration/expressions.html

"environment" == "Prod" ? 3 : 1

give above line in ec2 under count 
count = "${var.environment == "Prod" ? 3 : 1}"

in main.tf also count change to above count variable type
in public subnets && public route table assosiation creation count give likke
count = 1
terraform plan
terraform apply

after that (main.tf insted of static 1 we will give dynamic way)
count = "${var.environment == "Prod" ? 1 : 1}" (according to Prod ? 1 --> it will indicates the production how many servers we need to launch
terraform plan
terraform apply

---- lookup id done (depends on region we will launch the instances)
note: if u had change anything or any errors it will auto update the server we don't need to destroy and init it)
2 nd option --> taint
terraform state list
terraform taint aws_instance.public-instances.[0].id
terraform taint aws_instance.public-instances.[1].id
(above way also we deleted 2 instances of region)

-------------------------
null resource while running everytime terraform (without again creating instance the datafile will just append the data to that instance)

when terraform execution perform some actions like add some data we use localexec
ex; create a one seperate file like :nullresource.tf
local_exec
resource "null_resource" "localexec" {
    count = "${var.environment == "Prod" ? 1 : 1}"
    provisioner "local-exec" {
    command = <<-EOF
      echo "${element(aws_instance.public-instanes.*.public_ip, count.index)}"  >> details.txt
    EOF
    }
}

terraform init
terraform plan
terraform apply

remote_exec
when you want acees remote servers and perform actions at that time remote_exec will be helpfull
above same null file only add below code
//same null file add the below code
** note must be careful with username and private pem key files
provisioner "file" {
    source      = "script.sh"
    destination = "/tmp/script.sh"
    connection {
    type     = "ssh"
    user     = "ec2-user"
    #password = "India@123456"
    private_key = "${file("LaptopKey.pem")}"
    host     = "${element(aws_instance.public-instanes.*.public_ip, count.index)}"
    }
    }
    
  provisioner "remote-exec" {
    inline = [
    	"sudo su"
	"cd"
      "chmod +x /tmp/script.sh",
      "sudo bash /tmp/script.sh",
    #   "sudo yum update -y",
    #   "sudo yum install nginx -y",
    #   "sudo service nginx start"

      ]
    connection {
    type     = "ssh"
    user     = "ec2-user"
    #password = "India@123456"
    private_key = "${file("LaptopKey.pem")}"
    host     = "${element(aws_instance.public-instanes.*.public_ip, count.index)}"
    }
    }
    


the script.sh file will be like this
crate one file like script.sh paste below content
#!/bin/bash
echo "Welcome To DevOps Training By Sreeharsha Batch 13..!!"
and create one pemfile
east111.pem




**if u had any errors follow below procedure
terraform state list
terraform taint null_resource.localexec[0]
terraform apply

now we r going to null resource

what is null resource:
======================== complete null file=========
resource "null_resource" "localexec" {
    count = "${var.environment == "Prod" ? 1 : 1}"
    provisioner "file" {
    source      = "script.sh"
    destination = "/tmp/script.sh"
    connection {
    type     = "ssh"
    user     = "ubuntu"
    #password = "India@123456"
    private_key = "${file("east111.pem")}"
    host     = "${element(aws_instance.public-instanes.*.public_ip, count.index)}"
    }
    }

    provisioner "local-exec" {
    command = <<-EOF
      echo "${element(aws_instance.public-instanes.*.public_ip, count.index)}"  >> details.txt
    EOF
    }

    provisioner "remote-exec" {
    inline = [
      "chmod +x /tmp/script.sh",
      "sudo bash /tmp/script.sh",
      "sudo yum update -y",
      "sudo yum install nginx -y",
      "sudo service nginx start"

      ]
    connection {
    type     = "ssh"
    user     = "ubuntu"
    #password = "India@123456"
    private_key = "${file("east111.pem")}"
    host     = "${element(aws_instance.public-instanes.*.public_ip, count.index)}"
    }
    }


}

============================null file ends ============================================================================================


========foreach function====================

it is introtuced in terraform version 12.04
ref url: * https://github.com/rajeshsingamsetti/Devopscompletetuts/tree/main/Terraformforeach
https://www.hashicorp.com/blog/hashicorp-terraform-0-12-preview-for-and-for-each

* https://www.bogotobogo.com/DevOps/Terraform/Terraform-Introduction-AWS-loops.php

*** diff between foreach and count
reurl: https://medium.com/@business_99069/terraform-count-vs-for-each-b7ada2c0b186

** in count if u change the list order in array after first time execution done  it will had some issues
in count if u change the order it will replace the entire code

to solve the above probem we use foreach
above url example if u change order of array also nothing will happen foreach will handle
create main.tf

#This Terraform Code Deploys Basic VPC Infra.
provider "aws" {
    access_key = "AKIAQ5WQEB7SAYWSLJZ7"
    secret_key = "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"
    region = "us-east-1"
}

create variables.tf

# variable "buckets" {
#     type = list(string)
#     default = ["devopsb13bucket4","devopsb13bucket1", "devopsb13bucket2", "devopsb13bucket3"]
# }

# locals {
#   #images = { for v in var.images : v => v }
#   mybucks = {
#       for bucket in var.buckets:
#            bucket => bucket
#   }
# }

# resource "aws_s3_bucket" "bucks1" {
#   count = "${length(var.buckets2)}"
#   bucket = var.buckets2[count.index]
#   acl    = "private"
# }

after terraform apply 

again u can change in list array any order foreach will handle but count it will through an error
** dump the count foreach code lets try on it

---------------------------
lifecycle
terraform imports
terraform cloud using modules

lifecycle
refurl:
create_before_destroy 
prevent_destroy 
ignore_changes 

resource "aws_instance" "example" {
  # ...

  lifecycle {
    prevent_destroy = true
    //create_before_destroy = true
  }
}

///////////// aws s3 creation using terraform

provider "aws" {
    region = "ap-south-1"
}

resource "aws_vpc" "mypc" {
  cidr_block = "10.0.0.0/16"
}


resource "aws_s3_bucket" "b" {
  bucket = "my-tf-test-bucketnewraj"
  acl    = "private"

  versioning {
    enabled = true
  }
}

# Upload an object
resource "aws_s3_bucket_object" "object" {

  bucket = aws_s3_bucket.b.id

  key    = "profile"

  acl    = "private"  # or can be "public-read"

  source = "raj.txt"

  //etag = filemd5("myfiles/yourfile.txt")

}

==================================================








































