aws => devops => azure => k8's => python

why aws
roadmap for aws carrer
aws account creation
iaas vs paas vs saas
check jobs available on aws

why aws 
good in iaas ,more jobs, rapid developement,Heavy automation tools

AWS Regions & Avaialability zones

ref url:
https://aws.amazon.com/about-aws/global-infrastructure/?p=ngi&loc=0
https://aws.amazon.com/about-aws/global-infrastructure/regions_az/

more graphical about az and regions
ref url:
https://infrastructure.aws/

az means region inside a place with in 100km a dataceneters will exist

3 tier or multi tier archetecture menans
we have one service like flipkart if we deploy that service in only 1 az that az gone means evertything is gone in aws we don't fallbackloops.

to overcome load or availability we need to deploy our webapp on multi az then 1 az gone means another az will available

virualization:
ec2 every machine is virutualization

physical server
hypervisor
webapps
db

dedicated physical servers high cost rarely using

======================================
IP Adressing
2 types
ipv4 32 bit size -- 
172.217.13.228
1.0.0.0 to 255.255.255.255
ipv6 64 bit --- 
now no one are using ipv6

mostly using ipv4
ip address class
refurl:
https://www.paessler.com/it-explained/ip-address

Class	Address range
Class A	1.0.0.1 to 126.255.255.254
private range:
10.0.0.0 – 10.255.255.255
all office private ip will be in 10 ranges (if we need more ip's we choose class a willgive 16 million hosts)
Class B	128.1.0.1 to 191.255.255.254
private range:
172.16.0.0 – 172.31.255.255
(class b having 65,000 ) ip address
Class C	192.0.1.1 to 223.255.254.254
private range:
192.168.0.0 – 192.168.255.255
class c having (254 ip addresss)
Class D	224.0.0.0 to 239.255.255.255

world every ip exist in above classes

 class a network is enterprice network
diff bw pubic & private ip
ex: 10 th hall ticket in ur own school rollno

*** we most lo enterprise we work on classa

private ip is internal network communication purpose(inside over network communication).
publicip is communicate over internet (all world public ip should be unique)
nat gateway is a router provide internet access to all machines
==========subneting===================
refurl: http://www.subnet-calculator.com/subnet.php?net_class=A
000 - 1
001 - 2
010 - 4
011 - 8
100 -16
101- 32
110 - 64
111 - 128

if we add all bits we will get 255 bit
(subneting)
ref url of subnet calculator: http://www.subnet-calculator.com/subnet.php?net_class=A
10.1.0.0/16 > 65k
10.1.1.0/24 > 255 
10.1.2.0/24 > 255 

if i need only 32 ips we will go with above calculator
and network class is check A --> ip address:10.1.1.0 and under host per subnet choose nearest value 32 is 64 thats why i am choosing 64 + subnet mask 255.255.255.0
again 2 nd subnet --> 10.1.1.64 --> subnet range 64  + + subnet mask 255.255.255.0

(don't more think about subnet just know 10.1.1.0/24 && 10.1.2.0/24

10.1.1.0/24 --> 254 ip's
   /25 (half of 23) --> 122
   /26 --> 60
   /27 --> 30 ip's
 -----------public cloud &&& private cloud difference----------
 public cloud everyone can acess --> aws,azure,
 (we work public servers in public network we are create own network layer is called as vpc)
 private cloud only with in a network we don't use it).
 
vpc use
custom own network layer or interface
more security

==================================================================vpc==============================================================================
BY DEFAULT WE can create 5 vpc only of we need more we need to send request to aws team

when we create vpc it will create its own layers and all the instances are present in network and all instances will communicate in same network layer .but different
network layeres we won't able to join .
note when we create subnet we don't had an chance to change
By default aws we will have an vpc
vpc archetecture 
ref url: https://d2908q01vomqb2.cloudfront.net/77de68daecd823babbb58edb1c8e14d7106e83bb/2018/03/22/VPC-Architecture-1.jpg
lets create a vpc
=================steps to create a vpc=============== (multiple vpc had same private id we don't have no problem bcoz vpc's isolated that too its going diff igw)

** main source: https://docs.google.com/document/d/1nt6h_cDQqESPaInvpvpKdtUqgd0e2N-ra61qZedVfbE/edit
**: 1 own 111 slide: https://docs.google.com/presentation/d/1jOk9XdxR35zI4sYA2vacSSwz8KGtvpFbs_Fk3Jti5yE/edit#slide=id.p
** 2 documentation: https://drive.google.com/file/d/1X73iKh-c-TXUvMd6TIfsGKKK9Z9fZ5TX/view?ths=true

1. Decide on VPC name and create a new VPC. For this class VPC name is AWS-JAN-21-VPC and IP Address space we are going to use is 10.1.0.0/16 =>65000 IPs. 
Enable DNS Hostnames.

2. Create three subnets in AWS-JAN-21.Enable automatic IP Assignment.(it will auto assign public ip's to ur subnet instances).
(subnet ranze should be unique)
JAN-21-Subnet1 - 10.1.1.0/24 -1a
JAN-21-Subnet2 - 10.1.2.0/24 -1b
JAN-21-Subnet3 - 10.1.3.0/24 -1c

3. Create internet gateway and attach it to VPC. IGW name is AWS-JAN-21-IGW.
*when vpc created default one route table will created
4. Create new routing table for AWS-JAN-21 and assosiate subnets to it. Ignore the default routing table.
edit routes --> add route 0.0.0.0 + internetgateway(choose select it). + save route
=subnet assosation --> add your subnets 
5. Add routes in the routing table towards Internet Gateway.

6. Create a security group Infy-VPC-SG and allow all in bound ports.

7. Create a Key-Pair which create Public and Private Keys. AWS will have Public Key and you will be given private key. We need to use the private key to login to the instance.

8. Deploy a Linux t2.micro instance and connect to it using private key u have created.

=======================================================================================================

detail explanation
Virtual private cloud . 10.2.1.0/24
1. Decide what IP Range we need to use : 10.0.0.0/16 (255.255.0.0)
2. Create a VPC and name it as rajvpc. ENable DNS Hostnames.
3. Create 3 Subnets as below:(subnet ranze should be unique)
   raj-Subnet-1 - 10.1.1.0/24 - us-east-1a
   raj-Subnet-2 - 10.1.2.0/24 - us-east-1b
   Ford-Subnet-3 - 10.1.3.0/24 - us-east-1c
Once Subnets are created, enable automatic public ip assignment.
4. Create internet gateway using Name Ford-IGW and attach to VPC.
5. Create a new routing table as Ford-Main-Table and attach Subnets to it.
6. Create a route towards IGW.
7. Create a Security Group and allow all ports.
8. Create a Key Pair.
9. Deploy a new VM/Instance/Server and connect using the key u created in step 8.


Step2:as follows click on create.
*** choose ur vpc actions → edit dns hostname → enable→ save that vpx
NoteWhen we are creating a vpc then default some route and network acls and security groups are created.
.
Step 3. Create 3 Subnets as below:
   raj-Subnet-1 - 10.0.1.0/24 - us-east-1a
   raj-Subnet-2 - 10.0.2.0/24 - us-east-1b
   raj-Subnet-3 - 10.0.3.0/24 - us-east-1c
Once Subnets are created, enable automatic public ip assignment.
Here we need to add 3 subnets with different availabilty zones and different cidr.



Now our vpc is present on 3 availability zones of n.verginia region.
Note: when u created subnetes ut will take as az to that region.
*** after all subenets created → check ur each and every subnet → modify auto assign ipaddress→ check on enable auto assign ipaddress.
4. Create internet gateway using Name raj-IGW and attach to VPC
Note : if u want get access your vpc we need Internet Gateway(IGW)
Go to internetgateway → create → give ur igw name.

After creating ur gateway check on ur gateway and actions choose ur attach vpc → search ur vpc name → attach it.

Note: we only attach one internet gateway to one vpc .(we can’t attach multiple igw to 1 VPC).

 5. Create a new routing table as Ford-Main-Table and attach Subnets to it.
By using routes we can guide them how it will go outside.

Click on route tables → create a route table → name rajroute → choose rajvpc → click on create.
5. Create a new routing table as Ford-Main-Table and attach Subnets to it.


After creating ur route table → check ur route name → below tab click on subnet association → here u will see all the subnets. → edit → check 3 subnets → click on save.

Above setup we did for internal communication thats why we add routes to subnet.
6. Create a route towards IGW.
If we want access from outside we need tell that routes when we access from outside follow this procedure for that → 
Check ur routes below click on routes tab → click on edit routes → add rule → in destination (0.0.0.0/0) + target (internet gateway click on it + choose ur gateway) → click on save.


7. Create a Security Group name rajvpc and allow all ports.
Now it wiil allow all traffic from outside for securing purpose we are going to add the security groups .
Create a security group → give name of security group → and under vpc + choose our vpc → inbound rules (for demo purpose all traffic in real time don’t use all traffic) →
Create security group.


8. Create a Key Pair. (create a ur own key )
As of now i am using my default key.
For that go to ec2 → create your own key.

DEploy a linux micro instance ..


Under configure details follow above procedure.--> security groups choose ur rajvpc security groups → key pair don’t have create it or exist use it → launch instance.

After go to the terminal using mobxtream or putty.using public ip address.

If its logged in means successfully we configure vpc.
===========================================================================

                                                                    VPC PEERING
To connecting 2 vpc’s called as the vpc peering.

Previously vpc peering available in same region but now vpc peering available on across all regions

2 types:
Inside region
Outside region

Uses :
Single company.
Testing development and production
Multi companies:
We can peering b/w employees can access other company servers and vice versa
For security purpose:

If u want access any vpc like testing or dev first u need to managed or secured by management vpc.
Hands on create 3 vpcs on 2 vpc or in one region and 1 vpc in different region.
Ex:n.vieginia 2 vpcs and mumbai 1 vpc.





After go to ec2 and launch the instances.
Ec2 → micro → configure → choose rajvpc1 → choose ur subnet → practise purpose (security group allow all) → launch instance..
Same above process launch other instance in rajvpcsameregion

Login 2 servers of same region in terminal.
Login in to 2 servers ping from one server to another server of private ip not pulicip.
Note: when private ip’s are pinging then vpc peering done successfully.not public

Initially it wont ping after we did vpc peer it will ping.

For vpc peering.
Note: vpc peering 2 servers don’t have same ip if exists it will through an error
Go to aws -> vpc connection → create a peering connection → tagname (rajpeering to rajvpc1) →  select under local vpc under → choose the vpc of rajvpc → select another vpc accepter → choose our another (sameregionvpc) → create a vpc peering.

Afte it will show pending for that → actions → accept request → ok.--> now the status will be active .

 .




After that go to rajvpc instance terminal ping another server private ip it won’t work.
Because we need to update the route table. Of 2 vpc routes
Now first  go to rajvpc routes → below routes tab → click on edit → under destination u 

have to give the another server vpc range (means sameregion vpc range is 10.0.1.0/16) → target choose peer connection (chooose raj peer connection ) → click on ok.

Now go to route of rajsameregion check this and below roues add destination ip is (the ip range of rajvpc 10.0.0.0/16) → target + peerconnection choose raj peer connection → click on ok.

After all settings done gop to rajvpc ec2 terminal nad ping the private ip address of rajsameregion (second server ) now it will ping the private ip address.

Note: if u want check this doing exactly peering or not just go to vpc routes → edit ---> under destination (change the ip range from default value to anything it will stop the pinging again u revert back it will start to pinging once).

As of Now above we did only in same region if that vpc there in out of region it will be chargeable follow below procedure.

NOte: of u want perform multi region peering the keypair of ec2 instance login it must be same.
For that go to mumbai region → under ec2 → keypair → actons → import → place the ppk file of other instance or → under public key contents + copy the instance public key below procedure as follows. → click on import.

If u want get a public key login into server → ec2-metadata (under last we will get ssh-rsa public key copy that and .

Launch the instances.
Launch instance in mumbai region → choose key what we configure key.

Creata peering connection → name northverginia instance to mumbai → choose nvregionvpc (as a requester) → under select another vpc peer with + region tab choose another region choose mumbai region → under vpc acceptance (input field copy the vpc id of mumbai region vpc) go to mumbai vpc check below summary u will find vpcid →paste in acepter vpc → create peering connection .

****** if u want activate the peering connection go to your destination peerconnection region  → under peer connection action u will find the activate peer connection.(this connection for actions not showing enable peerconnection on source vpc).


Now login into two terminals try to ping it won’t ping now we need to configure the 2 routing tables.

Routing tables -> edit routes → destination + give mumbai ip range ) → target -->choose vpc peerconnection choose ur peer connection).

Routing tables -> edit routes → destination + give mumbai ip range ) → target -->choose vpc peerconnection choose ur peer connection)

Now u can go to another server routes follow above the same procedure now it will pingf.

Now not ping now i want access the nv server from my mumbai server for that we need to configure pemfile.

Login into mumbai server as a root → sudu su -
After create a file with same name → vi sp-key.pem (paste the key pair pem file total code in this file).
Note: keypair should be common between both servers,
After pasting the pem file now from ur root of mumbai server uf u access means it will through an error.
Bcoz we need provide read permission to pemfile for that
chmod 400 sp-key.pem

Now from root try like this below way.
root@ip-10.64.63]#   ssh -i sp-key.pem ec2-user@privateipofserver + enter 

Now u can successfully login into that server from mumbai.
Note : if u want access tha another server peering must be enable.

*** when vpc concept → ip’s are must be different otherwise ip’s are overlap.
Vpc transit won’t work.
******** ec2-metadata copy key and paste in all key files .
note : another region and another account same key file will be needed.

============================ aws vpc nat gateway===================================================
what is nat gateway:
NAT Gateway, also known as Network Address Translation Gateway, is used to enable instances present in a private subnet to help connect to the internet or AWS services. In addition to this, the gateway makes sure that the internet doesn’t initiate a connection with the instances.
NAT Gateway service is a fully managed service by Amazon, that doesn’t require any efforts from the administrator.

They don’t support IPV4 traffic. In the case of IPV4 traffic, an egress-only internet gateway needs to be used (which is another service).

A NAT gateway in a device forwards the traffic from instances present in the private subnet to the internet/AWS services, and sends back the response from the server back to the instance. When the traffic moves to the internet, 
an IPV4 address gets replaced with the NAT’s device address.
Once the response is obtained, it has to be sent to the instance, and in this case, the NAT device translates the address back to the IPV4 and it is given to the IPV4 address.

There are two kinds of NAT devices which AWS offers- A NAT gateway and a NAT instance. AWS recommends the usage of NAT gateways since it helps provide high availability and a better bandwidth in comparison to NAT instance.


ref url: https://www.knowledgehut.com/tutorials/aws/aws-nat-gateway

when vpc is created default route table is created (if u are not created ur route it will use default route)

****note: which subnet is not directlly connected to igw is called private subnet


to implement nat we need one vpc create with ip range 192.168.0.0/16
Note : after created subnets must add public ip allication
after create 2 public subnets diff region with range on 192.168.1.0/24 && 192.168.2.0/24 
after create 2 private subnets diff region (is same region diff iprange)with range on 192.168.10.0/24 && 192.168.20.0/24

note : default one route table is created it assigns all your subnets
crete one internet gateway attach to ur vpc

go to default created mainroute and edit routes add internet gateway 0.0.0.0 choose igw

but here if u observe all subnets means private subnets also connected to igw

now create one route table
choose your vpc crate after come to subnet assosiation
choose your private subnets and add it

now ur private subnets or real private subnets

create one security group with allow all trafic

now launch 2 ec2 instances one for public another for private

launch instance choose public subnet and create it launch it

now launch another instnce with private subnet (wnatendly choose public ip enable)

if you ping public ip it will ping but if u come to private it won't be ping(private server private ip check with it) 


Now for private subnets we create one nat gateway

create a nategateway
choose one public subnet and choose public instance allocate elastic ip: create it

now you can login into ur above attached public instance and ping ur private instance privateip it will ping

create one file in public instance
nano somekey.pem
copy paste ur pem file(which u choose for private instance)
chmod 400 somekey.pem

now we r try to login private subnet

ssh -i somekey.pem ec2-user@ipaddressprivate
after loginto private serverr
if u ping www.google.com
it won't ping bcoz it don't have internet access

** now go to nat private routes edit routes + add route
0.0.0.0 and choose target nat gateway is ur natgateway name

now u ping private instance ping www.google.com


*** note if u want to private server use internet we must use nat gateway


=================== elastic ip==========================================================

if you stop and  start ur instance ur ip will change if u want overcome that u need to use elastic ip(contant allocated ip won't change)

create elastic ip
assosiate elatic ip to ur running instances and click on assosite

now u reboot ur system and start the instance it will constant ur ip adress it won't change ur ip

elastic means we will change elastic ip from one server another server (dis assosiate and choose another instance assosiate it)
we will use in elastic ip
nat gateway
transit gateway
public web server
load balancer

============================================================End Points==============================================================
if u want your access ur service internal and do not access ur services on internet or external then we choose endpoints

launch one instance and create one user and aws configure

enter access key and secreat key

above procedure for user natgateway puprpose through internet not internal or insidw



Endpoints
2 types of endpoints

create a endpoint --> 2type of services

for only dynamodb && s3 only had gateway  remaining all are interface

Interface endpoints

An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet. 
It serves as an entry point for traffic destined to a supported AWS service or a VPC endpoint service. Interface endpoints are powered by AWS PrivateLink.

For information about the AWS services that integrate with AWS PrivateLink, see AWS services that you can use with AWS PrivateLink. 
You can also view all of the available AWS service names. For more information, see Viewing available AWS service names.

Gateway Load Balancer endpoints

A Gateway Load Balancer endpoint is an elastic network interface with a private IP address from the IP address range of your subnet. Gateway Load Balancer endpoints are powered by AWS PrivateLink. This type of endpoint serves as an entry point to intercept traffic and route it to a service that you've configured using Gateway Load Balancers, for example, for security inspection. You specify a Gateway Load Balancer endpoint as a target for a route in a route table. Gateway Load Balancer endpoints are supported for endpoint services that are configured for Gateway Load Balancers only.

Gateway endpoints

A gateway endpoint is for supported for AWS services only. You specify a gateway endpoint as a route table target for traffic destined to the following AWS services:

Amazon S3

DynamoDB


***** practical
**** for entrypoint s3bucket and vpc shold be in same region
before assign iam role or configure aws on instances

create one entrypoint + search for s3 + check s3 + choose your vpc  + choose ur route + create entry point

launch that above selected public or private region subnet instance .

after the entrypoint creation id will affect on routes of above subnet u choose in entrypoint
launch instance 

in ur aws account s3 create or upload files

wget filename u will get that file

upload files

i=0
[root@ip-10-0-0-30 ~]# while [ $i -lt 3 ]
> do
> echo $(date) > file$i
> aws s3 cp file$i s3://bucketsufit
> i=$(( $i + 1 ))
> sleep 1
> echo "copying file file$i"
> done


interface environment

more focus on session manager:
for private subnets neeeds to practice
ref url:
https://aws.amazon.com/premiumsupport/knowledge-center/ec2-systems-manager-vpc-endpoints/

=======================================================security groups and nacl's======================================================
inbound (outside traffic comming to inside)
outside (inside machine traffic going outside)

firewal types
statefull (one side traffic allows otherside it will auto allows the traffic) ex: security groups
stateless ---> nacl

to test the server we use telnet ipaddress portno don't use ping


in security groups source we will add subnets or multiple ip's

note : network acl we don't use in real time.
nacl for subnet level purpose
nacl don't do change
when vpc created nacl are created defaultly


note :nacl inbound outbound never will be in same port

outbound rule custom + port range is 49152-65535

================== transit gateway==================================

defination:

Transit Gateway is an awesome feature announced by Amazon Web Service to simplify the network connectivity. 
In the previous scenario, you had to use VPC Peering or Transit VPC concept to form transitive nature. Using peering connection you had to implement a 
full mesh topology 
with the AWS VPC’s to make communication between each other. In VPC Peering you can’t create or connect VPN connections with an on-premises network

trasit gateway supports multi region

it will charge

create a transit gateway
name + check auto accept shared attachments create trasit gateway

launch 3 instances 3 dif vpc

ping from one instance to other instance it won't ping bcoz diff vpc


now click on transit gateway attachments + choose ur transit gateway + give name + choose vpc + check which region u allocated + create it 
choose another vpc
now click on transit gateway attachments + choose ur transit gateway + give name + choose vpc + check which region u allocated + create it 
choose another vpc
now click on transit gateway attachments + choose ur transit gateway + give name + choose vpc + check which region u allocated + create it 



wait for all state status will change the status to available

now we need configure all the routes 
go to each everyone route table
edit routes +
destination place other 2 vpc ip range + target choose transit gateway ur trasit gateway + save routes

do same operation in all other vpc's


we can do another region also vpc connection using tgw peering

================================== ec2=============================

amimachinename --(will change from region to region)
myimi --> will use in real time(bcoz its custumizable)

Here choose ami and add security group and launch the instance here we can stop and start the instance.

To connect ec2 instance through ssh follow this procedure:

ssh ec2-user@public ip address

Ur downloaded key in .pem file
Now we need change the permissions for our pem file
chmod 0400 mykey.pem
ssh -i mykey.pem  ec2-user@public ip address
Type yes and it will connect

Now u r successfully logged in into ur account.

After logged in just type 
whoami (it will display username ec2-user)

Above for mac and linux;
For windows we can use :
Puttygen software :
Download by using this u can connect to server.


Security Groups
• Can be attached to multiple instances
• Locked down to a region / VPC combination
• Does live “outside” the EC2 – if traffic is blocked the EC2 instance won’t see it
• It’s good to maintain one separate security group for SSH access
• If your application is not accessible (time out), then it’s a security group issue
• If your application gives a “connection refused“ error, then it’s an application
error or it’s not launched
• All inbound traffic is blocked by default
• All outbound traffic is authorised by default

Private vs public ip:
• Public IP:
• Public IP means the machine can be identified on the internet (WWW)
• Must be unique across the whole web (not two machines can have the same public IP).
• Can be geo-located easily
• Private IP:
• Private IP means the machine can only be identified on a private network only
• The IP must be unique across the private network
• BUT two different private networks (two companies) can have the same IPs.
• Machines connect to WWW using a NAT + internet gateway (a proxy)
• Only a specified range of IPs can be used as private IP

Elastic IPs

• When you stop and then start an EC2 instance, it can change its public
IP.
• If you need to have a fixed public IP for your instance, you need an
Elastic IP
• An Elastic IP is a public IPv4 IP you own as long as you don’t delete it
• You can attach it to one instance at a time
 With an Elastic IP address, you can mask the failure of an instance or software
by rapidly remapping the address to another instance in your account.
• You can only have 5 Elastic IP in your account (you can ask AWS to increase
that).
• Overall, try to avoid using Elastic IP:
• They often reflect poor architectural decisions
• Instead, use a random public IP and register a DNS name to it
• Or, as we’ll see later, use a Load Balancer and don’t use a public IP

Hands on → left side click on elastic ip → allocate ip address → choose Amazon's pool of IPv4 addresses → allocate → choose ec2 instance ip address → actions → associate elastic ip address  → choose instance select ur instance  → click on associate.
Note: once elastic ip created we need to associate any ip address otherwise we will get the bill.
For free tier we can use 5 elastic ip address. 

EC2 User Data(BootStrap

• It is possible to bootstrap our instances using an EC2 User data script.
• bootstrapping means launching commands when a machine starts
• That script is only run once at the instance first start
• EC2 user data is used to automate boot tasks such as:
• Installing updates
• Installing software
• Downloading common files from the internet
• Anything you can think of
• The EC2 User Data Script runs with the root user
We want to make sure that this EC2 instance has an Apache HTTP
server installed on it – to display a simple web page
• For it, we are going to write a user-data script.
• This script will be executed at the first boot of the instance. 

When we installing a server: under configure instance category we saw user data we need to just  write
#!/bin/bash
sudo su
yum update -y
yum install -y httpd.x86_64
systemctl start httpd.service  
systemctl enable httpd.service  
cd /var/www/html
echo "<html><h1>This is WebServer 01</h1></html>" > index.html

EC2 Instance Launch Types
• On Demand Instances: short workload, predictable pricing
• Reserved: (MINIMUM 1 year)
• Reserved Instances: long workloads
• Convertible Reserved Instances: long workloads with flexible instances
• Scheduled Reserved Instances: example – every Thursday between 3 and 6 pm
• Spot Instances: short workloads, for cheap, can lose instances (less reliable)
• Dedicated Instances: no other customers will share your hardware
• Dedicated Hosts: book an entire physical server, control instance placement
 
EC2 On Demand
• Pay for what you use (billing per second, after the first minute)
• Has the highest cost but no upfront payment
• No long term commitment
• Recommended for short-term and un-interrupted workloads, where
you can't predict how the application will behave.

EC2 Reserved Instances • 
Up to 75% discount compared to On-demand 
• Pay upfront for what you use with long term commitment 
• Reservation period can be 1 or 3 years 
• Reserve a specific instance type 
• Recommended for steady state usage applications (think database) 
• Convertible Reserved Instance 
	• can change the EC2 instance type 
	• Up to 54% discount 
• Scheduled Reserved Instances 
• launch within time window you reserve 
• When you require a fraction of day / week / month



EC2 Spot Instances
• Can get a discount of up to 90% compared to On-demand
• Instances that you can “lose” at any point of time if your max price is less than the
current spot price
• The MOST cost-efficient instances in AWS
• Useful for workloads that are resilient to failure
• Batch jobs
• Data analysis
• Image processing
• Not great for critical jobs or databases

• Great combo: Reserved Instances for baseline + On-Demand & Spot for peaks

EC2 Dedicated Hosts
• Physical dedicated EC2 server for your use
• Full control of EC2 Instance placement
• Visibility into the underlying sockets / physical cores of the hardware
• Allocated for your account for a 3 year period reservation
• More expensive
• Useful for software that have complicated licensing model (BYOL – Bring Your Own License)
• Or for companies that have strong regulatory or compliance needs

EC2 Dedicated Instances • Instances running on
hardware that’s dedicated to
you
• May share hardware with
other instances in same
account
• No control over instance
placement (can move
hardware after Stop / Start)

***ref url:  https://www.ec2instances.info
EC2 Instance Types – Main ones
• R: applications that needs a lot of RAM – in-memory caches
• C: applications that needs good CPU – compute / databases
• M: applications that are balanced (think “medium”) – general / web app
• I: applications that need good local I/O (instance storage) – databases
• G: applications that need a GPU – video rendering / machine learning
• T2 / T3: burstable instances (up to a capacity)
• T2 / T3 - unlimited: unlimited burst
• Real-world tip: use https://www.ec2instances.info


Ami HandsOn

Create ur instance or existing instance also will works on ami

Choose instance → click on instance →choose create image → give image name & description.--> click on create image.
Now under AMI’s → click on that ur ami → copy → choose whatever region u want → click on launch

Now our ami will launch the same configurations had previous instance.

Cross Account ami copy:
Choose ur ami → modify image permissions → choose private → here u can provide other aws account number

Placements Groups
 Under ec2 left side we can see that → placement groups → give name → cluster → create.

Create another placement group with : placement groups → give name → spread → create.
Create another placement group with : placement groups → give name → partition → 3 → create.

Create instance → in configure instance we can see that placement group option → existing → example i am taking cluster → next → crete instance

Elastic Network Interface

Eni is a virtual network interface .
In under ec2 -> network interface we can create our own interface.
We can add custom network interface to existing instance .
Same instance we have multiple networks.

Hands on:
Create network interface → give discription and select availability zone. → click on cretae.

Basically before it will be in available .click on that → right click → attach instance → choose instance → click on attach.

Note: 1 instance having and 2 network interface.

Whenever u want detach to instance and attach to other instance

Ec2 Hibernate:

• We know we can stop, terminate instances
• Stop: the data on disk (EBS) is kept intact in the next start
• Terminate: any EBS volumes (root) also set-up to be destroyed is lost
• On start, the following happens:
• First start: the OS boots & the EC2 User Data script is run
• Following starts: the OS boots up
• Then your application starts, caches get warmed up, and that can take time!

• Introducing EC2 Hibernate:
• The in-memory (RAM) state is preserved
• The instance boot is much faster!
(the OS is not stopped / restarted)
• Under the hood: the RAM state is written
to a file in the root EBS volume
• The root EBS volume must be encrypted
• Use cases:
• long-running processing
• saving the RAM state
• services that take time to initialize


Ec2 hibernate:(paid one)
Now ec2 hibernate supports on only amazon linux2 → instance type choose on M5 Large → now under configure instance we can see that → stop hibernate check hibernation on additional stop behaviour .

Stop and start instance we can see that start hibernate.
--------------------------------------------------------------------------------------------------
EC2 for Solutions Architects
• EC2 instances are billed by the second, t2.micro is free tier
• On Linux / Mac we use SSH, on Windows we use Putty
• SSH is on port 22, lock down the security group to your IP
• Timeout issues => Security groups issues
• Permission issues on the SSH key => run “chmod 0400”
• Security Groups can reference other Security Groups instead of IP
ranges (very popular exam question)
• Know the difference between Private, Public and Elastic IP
• You can customize an EC2 instance at boot time using EC2 User Data
• Know the 4 EC2 launch modes:
• On demand
• Reserved
• Spot instances
• Dedicated Hosts
• Know the basic instance types: R,C,M,I,G, T2/T3 (ram,cpu,medium ,IO, GPU )
• You can create AMIs to pre-install software on your EC2 => faster boot
• AMI can be copied across regions and accounts
• EC2 instances can be started in placement groups:
• Cluster
• Spread

======================================  ec2 ebs volume======================================================================
Storage types
Storage are 2 types 1.ebs 2. Instance store 3. Elastic file system

In ebs we need to format the disk and create file system on it.
But in efs : file store - acess the network ans then mount it and save the files .no need to formating creating a file system

Other storages are 
S3
Glacier

Ebs features:
Persistance or permanent storage
Ebs volumes will be created but not decreased
We can have the volume between 1gb to 16tb

Handson : deploy a machine → (note if u want instance store choose m3.medium with 1*4 ssd )-->under add add storage (root size is 8gb ) create one ebs volume with 3gb → launch the instance.
Go to terminal:
sudo su && cd
lsblk (here we can see that all our storage information and it is mounted or not we can see that)
Note: after creating a instance also we can create a volume and we can attach it also.

But we can’t use it bcoz we format it and we make file system on it and we mountit later only we can use in ebs volumes.
Now i am going  to create a directory with newebsvolume mkdir newebsvolume
First i need to format the newly creted volume for this 
fdisk /dev/xvdf (here xvdf is the name of newly created volume we can find using by lsblk command)
m after n (for new partition ) after press p after enter 3 times note when u see command u need to type w
Now if we can see lsblk now our new volume will be created a new partition.
As of now we converted our raw volume to partition volume for further usuage
Now we need to create a file system → mkfs.ext3 /dev/xvdf1 (xvdf1 is the name of partition volume)+ente
Now we are going to mount the volume.
mount /dev/xvdf1 /newebsvolume/ (here newebsvolume it is our newly created directory) + enter
Now type df -h (here u can see the space of your instance)
Now we have to configure our partion volumes in fstab (for permanet storage on restart server no loss info
vi /etc/fstab (in that file write below code)
/dev/xvdf1  /newebsvolume         ext3   defaults   0    0  (save the file)
Go to newebsvolume crete some files 
Stop and start your server or instance loginto your  console your data will be there)
***** note: if u r not mention ur partition details on fstab on start the server ur data will be their but it won’t show in under the command of df -h
Go to ec2 -> volumes → edit volumes → (u can increae the volume size but u can’t decrease)
Note if  u add extra size to your volumes u have to follow below procedure 

Type growpart /dev/xvdf 1
→ lsblk (check ur we can see increase partion)
→ but df -h (still showing previous volume we need to change it)
→ resize2fs /dev/xvdf1
→ now check df -h (the size will be increased).

---------------------------------------------------------------------------------------------------------------------------------------------------------------
Different types of volumes:
Create a instance → under add storage we can see that volume types (general purpose ssd gp2 default root one ) → add tags → launch the instance.

After instance created now i am going to create a volume
Ec2 → volumes → create a volume 
Gp2 → 1gb is min and 16tb is the maximum

** note iops is * 3 times os size of volume ex: size is 200 then iops is 600 it’ s for gp2
For gp2: min iops: 100
                Max: 10000
For io1: min 100
               Max: 32000
Iops: means input output per second
** but if u choose volume type ops ssd (io1) we can increase iops not depeding on size of volume 


ec2→ create a image → save in snapshot → launch one instance → using snapshot → go to terminal now whatever we had a packages in previous volumes that all will be present in newly created instance.

Efs : elastic file system (it's a multi availability zone)
It is network based
We need to pay how much we are using efs.

Aws console → type efs choose it ---> give a name and launch it.
After go to ec2 -> launch a normal 2 micro instance with security groups nfs with sssh 0.0.0., → go to instance console → now we have to mount efs file system to our instances.
refurL: https://docs.aws.amazon.com/efs/latest/ug/mounting-fs.html
sudo yum install -y amazon-efs-utils
mkdir efs
sudo mount -t efs -o tls fs-12345678:/ /mnt/efs

Cd efs
Create a file and sudo touch sample.txt


Its a shared file system as that both are in same network as we created a file in one instance it will be reflect to other instance tooooo.

********
ebs:
it can be increased max 16tb
ssd & hdd
• An EC2 machine loses its root volume (main drive) when it is manually terminated.
• Unexpected terminations might happen from time to time (AWS would email you)
• Sometimes, you need a way to store your instance data somewhere
• An EBS (Elastic Block Store) Volume is a network drive you can attach
 to your instances while they run
• It allows your instances to persist data (*** creted volume both are should be in same region)
Volume can increase but not decrese.(instance store we lost data when reboot server).
Hands on→
Ec2 → add storage → add a new volume +choose ebs + size 2gib (everything is default uncheck delete on termination) → add tags -> security group → review → launch.


instance store:
its temporory storage its delete when server reset
it cannot be increased or decresed
only ssd types available

********* from madhu volumes start============**********
create a volume --> type gp2+ size 2 + region 1b + create a volume
take instance attach above volume launch it
lsblk
we can see xvda & xvdb && xvdf

cd /media/ephermal10
mkdir folder1
cd folder1

vi create one file save it
(its a temporory storage)

here xvdf is ebs volume inthe format of raw starcture in intial stage
now we need to partion
fdisk /dev/xvdf enter 3 times last type w
now lsblk 
now xvdf is partied

now we are going to make a file sysyem
mkfs.ext4 /dev/xvdf1

now we need to do the mount

mkdir /myebsvol
df -h(it won't visible we need to mount the data)
mount /dev/xvdf1 /myebsvol/
now df -h (it's mounted)

mkdir ebsfol
cd ebsvol
vi create a file save

now u can stop and start ur instance

if u want save those files after reboot also(if u wont those below step it won't work)

vi /etc/fstab

vi /etc/fstab (in that file write below code)
/dev/xvdf1  /newebsvolume         ext3   defaults   0    0  (save the file)

save it


create a snapshot

save ami
you need to learn windows partition also


================= indepth volumes and snapshot in ec2=========
we mostly used in gp2 and provisioned iops here provisioned iops is the high performance iops

gp2 --> min 1 gb max 16tb (we can assign multiple gp2)
min iops : 100
max iops : 16000

for 1gb we will get 3iops

iops concept more r & d

provision io1 --> min 4 gib max 16384 gib
min 100iops max 64000 iops

but its too costly

launch one instance root mkdir snapshot
cd snapshot
create some files

go to volumes create the snapshot

now again go to server 
create one more directory create some files 

create same volume snapshot2

again go to server
create new folder place some files
create one snapshot


**** note when ur data lost and trying restoring data take ur latest snapshot and launch it

now go to snapshots (latest snapshots)
left click create volume + don't decrease the sixe increase if u want + name and value give it.create volume

after go to volumes u will see ur volume click on that volume to attach choose ur instance + click on attach

if u type lsblk in ur instance u will see that

newly added volume xvdf1 

create one filder and mount id
mkdir restore_volume
mount /dev/xvdf1  /restore_volume

ls restore_volume
cp -rf restore_volume/ /
all the folders of snapshots wll be placed on root

unmount /restore_volume/


** this way only backup or restore the data only not os
after detach ur volume and delete ur data
if u want os u need to copy the machine from ec2 instance

snapshot life cycle management:
depends upon time and requirement by using tags we can create snapshots by using this policy

resource type : choose volume .or if u want instance choose instance

================== route 53 ======================================
godaddy registartion
public dns
private dns
sub domains


now goto aws route 53 dashboard
https://console.aws.amazon.com/route53/v2/home#Dashboard

create hostazone + paste ur domain name .(call2ask.in)

after that it will give
Hosted zone details

now inur godaddy Nameservers click on change button
now nameserver (paste the aws route 53 name servers 4 )

click on save button


go to ec2 launch one instance

after launching ur instance

now go to route53
create record

give name server1 + ur value is public ip + click on create

server1.domainname is called as fqdn(fully qualified domain name)
--hn-- hostname
server1.call2ask.in. (using this u will login into ssh connection also)

nslookup server1.call2ask.in

install nginx
service start nginx

now copy ur domain name and paste in google it willl display ur web server

Note: apart from server1 if want use diff name for same ip we can use cname concept

create a record 

give new recordname and record type choose cname + value paste previous fully qualified domain name(server1.call2ask.in) + create record

now try with server2.call2ask.in (it also browse from same ip).

in real time we can use (outside server ) and internally another domain name

above name called as hostname resolution.

note**** in route whenver u create a record choose name server and paste nameserver give ip save it . --> create it
now if another made any changes it won't be conflict

*****
if u want use internally zone
created hosted zone  name raj.com + choose private + choose vpc + create
create a record set + give naame + type a ipv4 + value give ur instance private ip + create

nslookup privatedname name and browse also it will show
note it won't aceees from outside

we use in real time 
ansible puppet client nodes access.

cat /etc/hosts(here we will assign hosting names)

route 53c layers - level
physical layer -1
datalink layer -2
network layer - 3
transport layer - 4. network load balancer
session layer - 5
presentation layer -6
application layer -7 - application load balancer.

================================================== Load balancers ======================================================================================

network load balancer - layer 4 - for all
application load balancer - layer 7 - it will only specific for http and https
classic load balancer(old no one r using)


we need 3 servers with 3 diff az's

combine 3 servers with one target group

flow
internet --> route 53 --> network loadbalancer --> target groups --> instances

handson 


launch 3 servers with 3 diff az's with nginx

after launching 3 instances.

for understanding purpose login ssh to ever server
vi usr/share/nginx/html/index.html

change code like server1

same process for another 2 servers also


target group --> collection of instances & port number

create a target group
name + protocol tcp + vpc + port 80 --> create
after creating below click on targets + edit
choose any 2 servers + save it


create a load balancer --> network --> mynlb + scheme internet-facing + choose vpc and check always public subnets.+ (real time mention ur elastic ipaddress) assign by aws.
create.

configure roting --> choose ur existing target group + next + create.

now below load balancer it will give very long load balancer name.

to assign dns

route 53.
create hostedzone.
paste ur  aws hosted zones name servers godaddy name servers edit paste step by step.


route 53:
create a record set
name www
type cname + value ur load balancer dns name.

create. Or alias choose ur loadbalancer create.

wait for status health check will be healthy.


first check with load balancer dns name.
open in multiple tabs check difference.


after that try with route 53 dns name.

in target we will saw the status health and unhealth.


now we are going to learn ssl & tls certificates.


aws will give some cerificates.(its free work only for aws)
go to certificate manager + provisional certificate + get started + domainname + *.call2ask.in + dns validation + next review + create.

in new tab open 
copy the name and go to route 53 + paste the name remove last domain name + type cname + value aws certificate manager value + create.

now go to certificate manager + click on continue it will take 10 min to create.


after certificate issued .
now come to load balancers + click on listners + add listner + choose tls + under security policy choose ur polocy + forward to our load balancer name + default ssl 
certificate + choose our certificate + create

now under lister delete 80 tcp (we had now tls 443).

now in ur dnshostname on google after few meniutes
https://dnsname


cross zone load balancer:
it only one region based.
alwas target groups :same equally load balancer or if u inequality load balancer wants to balance traffic check on cross zone load balancing

=========================================== application load balancer===============
now we are going configure third party certificate tools for that use below website
we need one dnsname from rout53 from aws

https://csrgenerator.com/
fill the details

under company name paste dns name + 2048

generate + ******* it will give request key and private key store securely

after comodossistore.com + create account + all orders + active + reissue certificate + click on cname verification tab + paste the first security of ur pasted key
click on continue + no + continue + 
download certificate

rooute 53 + create record set + paste the name + cname + value paste it.

after it will issue the certificate (but this small charchable)

application for only web applications


launch 3 instances. with nginx custumize

create target groups name + instance + http + create
like that create 3 target groups.
now each target groups + bottom target edit + add the instance + add to register + save.

like that other instance too.

now create load balancing
application + name + vpc + public subnets + next configute settings + give security group + configure routing + target group + target name + next create.
it will take time

now in certificate manager 
extract certificate + in that crt files + aws certificate manager + import cerficate + paste the unzip data file domainname.crt data code under certificate body
+ paste private key + certificate chain bundle code paste here + review + import

after loading our load balancer.
route53 + name domain name + type ipv4 + alias yes + choose ur target + save record.



after under load balancer + listners + view edit rules + add rule + insert + condition path /movies + forward this to movies.+ save it
like that u can add paths

add listner change to https + forward to targer group + security policy + ssl certificate import certificate + save.


now it will now https.

============================== auto scaliing ==========
Auto scaling having 2 types:
Horizontal scaling
It create automatically new servers when it load rises 
Scalein: reducing
Scaleout: increasing server
Vertical scaling:
When it reach the limit we need to stop again we need to run in real time we don’t use the vertical we use only horizontal.

In real-life, the load on your websites and application can change
• In the cloud, you can create and get rid of servers very quickly
• The goal of an Auto Scaling Group (ASG) is to:
• Scale out (add EC2 instances) to match an increased load
• Scale in (remove EC2 instances) to match a decreased load
• Ensure we have a minimum and a maximum number of machines running • Automatically Register new instances to a load balancer

Hands on : launch template tells us how do we create ec2 instances
Before we need one application load balancer with run on dns we need to get an 503 error
Go and ec2 ---> ec2 → autoscaling → auto scaling groups click on that → click on auto scaling group → under field of auto scaling group name  give name myasg name→ under launch template click on create launch template → one page will open → create a template name give a name → description give what u want .
→ now under amazon machine image ami → you can choose amazon linux2 ami → instance type is t2.micro
→ keypair choose your key .--> networking platform default vpc → security groups choose our instances security group → advanced userdata:
#!/bin/bash
sudo su
yum update -y
yum install -y httpd.x86_64
systemctl start httpd.service  
systemctl enable httpd.service  
cd /var/www/html
echo "<html><h1>This is WebServer 01</h1></html>" > index.html

Create launch template → click on view launch template .

Now go to asg console .--> create auto scaling group name  give name → under launch template  choose ur previous template what we setup .

Click on next -->under network choose click all subnets → if u want purchase then go with purchase + select all subnets whatever present→ click on next → now we are at under load balancing click on check enable load balancing → choose application load balancer → choose your target group if it exists choose ur target group or else create one new target group → 
Choose that group → in health checks elastic load balancer → click on next → in next  page min capacity is one and maximum capacity in my case i am choosing 5 → scalling polices as of now none → click on next → 
Notifications we don’t need click on next → add tags → click on next → review it click on create new auto scaling group  .

It will show successfully created message

Now in our auto scaling group dashboard we can see that instances is 0 and status updating capacity .
Click on refresh
Check asg under group details we can see that all information 
In under below we can see that instance management click on that we can see that one instance is created go to ec2 and check if u want.
That instances will display on under target groups section also with status of healthy
In autoscalling groups if you increase the desired capacity it will automatically crate new instance

from madhu: auto scalling(unpredictable loads)
autoscalling not used for databases it used for web & app servers bcoz of static content.

internet + route53 + network load balancer -->  web server1 + webserver2.
types of scalling:
vertical scalling : upgrading servers ( we need to stop/shutdown and update servers) for live servers we don't use in vertical scalling.testing environment we can use

Horizantal scalling : same server adding multiple times.not upgrading.(its costly)

launch ubuntu 16.04 instance .
login to server
update and apt install nginx stress git -y
service start nginx
systemctl enable nginx
(chkconfig nginx on)

after come to ec2 dashboard --> left click --> create image --> img name + description --> create image.

it will be there on under ami's

+
create a target group --> giva name + check instance + protocol is "tcp" + port 80 + choose ur vpc. --> create.

after creating target group --> go and edit it --> choose edit attributes change 300 to 60 seconds. + save.

NOW "create a load balancer"
choose network load balancer + give name + choose vpc + choose or check all public subnets + existing target group +choose u created target group +
review + create.

" now click on side menu launch configuration "
create + choose u r own ami starting what we created + select + next + give name lc1 + monitoring enable check + userdata give some script (note we will give wron)
aboove we given wrong or we need to update userdata later.
+ storage + review and launch.


go to side menu.
crete a autoscalling group + choose launch configuration + choose our launch configuration name is lc1 + next + name give autoscale name + group size (when u start 
how many instances u will start) i will choose 1 + choose ur vpc + choose ur subnets + advances details click on it.
+ check receive traffic from one more loadbalancer + target groups choose ur target group + helath check type "elb" it will monitor ur instance + grace period 120
seconds + next + check on use scalling polices to adjust the capacity of group .
scale between 1 to 3 + now click on scale the auto scalling group using simple polocy + now it will show increase the group size and decrease the group size

now in increase group size + add take action 2 (click on new alaram) unheck the notifications + whenevr maximum utilization of cpu >= 50% for least at 1 periods of 1 min
create alaram.
+ when 50 + add step + cpu utilization <= 75 add 1 instances + instances need 60.

now decrese alaram:
create alarm unheck the notifications + <= 40 .
take action remove + 2(instances) when 40  

review create + tags + create .

now minium instances will create on ur instances dashboard when autoscalling is created.

after go to autoscalling groups + edit details + under termination policies remove default select newest instances + save.

go to route 53 + choose our hosted zone domain name + type ipv4 address + target choose ur target group name + save record.

come to load balancer copy the dns name and paste in chrome .(it won't append ur userdata bcoz we given wrong to overcome that)
#!/bin/bash
apt update
apt install software-properties-common -y
apt-add-repository --yes --update ppa:ansible/ansible
apt update
apt install ansible -y
apt install python-apt -y
git clone https://github.com/mavrick202/ansibletemplatetesting.git /myrepo
ansible-playbook /myrepo/playbook.yaml



go to launch configuration + select our lc1 + copy to launch template + give name lc2+ launch
after edit lc2 + modify + advance + userdata copy the above userdata. + create a launch template.

create a launch configuration + our ami + name lc1 update + userdata paste ur userdata + review and launch.

come auto scalling group + right click edit + choose ur updated launch configuration lc1update + save 

come to ec2 instance dashboard .
terminate previously created instances.
now it will create new instance using new launch configuration.

we will observe in autoscalling.
instance auto update on target group also.

to obersve autoscalinng .
go to windows terminal
while true
do
curl -SL http://www.domainname.com/ /grep -i 'IP A'
sleep 2
done


ip address will change.



now login into all servers and make the load  stress.
stress --cpu 8 --io 4 --vm-bytes 128M --timeout 30m
run this command in all servers it will make pressure on all servers.

after load increase it will launch new instances.

if load stop automatically it will stop the instances


=========================== video 20 load balancer on private subnets using nat gateway==========================

Note: load balancer must be in public subnets
internet + dns + network load balancer + nat gatewat + public subnet + private subnet.



launch one nat gateway.+ choose one public subnet + allicate ip + create a nat gateway.
routing + 0.0.0.0/0 (nat gateway + choose ur nat gateway) ok

create a target group . choose tcp
create a network load balancer + under availability zones choose ur vpc +
under subnets + choose all public subnets.
+ configure settings + configure routing + existing target group + ur target group name + create.

now launch ubuntu instances with private subnets + with below userdata + 
#!/bin/bash
apt update
apt install software-properties-common -y
apt-add-repository --yes --update ppa:ansible/ansible
apt update
apt install ansible -y
apt install python-apt -y
git clone https://github.com/mavrick202/ansibletemplatetesting.git /myrepo
ansible-playbook /myrepo/playbook.yaml


+ create + launchit like wise launch 2 private instances.

copy private ip address paste in chrome it will show some design with ur ipaddress.


********* in realtime 3 tier application will be in below way

load balancer + web server + app server + database.
  
  |.            |
  |             |
  Public.      private.       private.  private
  

load balancer : copy the dns name paste in google
it will access in internet eventhough it will in private instances

nslookup dnsname

note : public ip will chaneg if u don't want to change u have to use elastic ip while u create loadbalancer + choosing subnets choose elastic ip 




note : when route53 if u use ip it will change if u use elastic ip it won;t change i.e we usinhg dns name

* internal load balancer research and implement it.



===================================================IAM Identity & Access Management======================================

groups.  -- for admin & developers contain id and password
uses.    -- for admin & developers contain access key and secreat key

Roles -- for aws services (to interact one aws service to another service from one account to another account)

Polices -- permissions in json format.
      types 3:
      aws managed polocies
      cutomer managed policies
      inline policies. (only for users and groups the polocy will exist but it won't display)
   reference i am polocy simulator.
   
create 2 users with programatic + aws console access + with cutom password.


if u want aceess aws on linux u neeed aws cli
for that u need to install aws cli

aws configure
accesss key + secreat key + region + json.

cat ./aws/credentials ( u will get ur aws details)

aws ec2 describe instances
u will get error like u donlt have an permission

to attach policy we don't use default policies we use custom polocies.

aws ec2 describe instances --output table
aws ec2 describe instances --output text

now we have 2 servers note: Owener testuser1 (its a tag)
that 2 servers can access 2 diff test users note: users can stop/start and reboot only don't had a chance to terminate.


by using tags we can write the policies.
create a polocy json polocy.

{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Effect":"Allow",
         "Action":"ec2:Describe*",
         "Resource":"*"
      },
      {
         "Effect":"Allow",
         "Action":[
            "ec2:StartInstances",
            "ec2:StopInstances",
            "ec2:RebootInstances"
         ],
         "Resource":[
            "arn:aws:ec2:us-east-1:accountnumber:instance/*"
         ],
         "Condition":{
            "StringEquals":{
               "ec2:ResourceTag/Owner":"Testuser1". // here we can add tags 
            }
         }
      }
   ]
}


after creating polocies attach that polocies to users


=============== aws s3=============
s3 is not global (it will list the all regions buckets true global is iam)

s3 availability(monthly uptime percentage is 99.9%)


s3 durability(if a placed in s3 how mush it safe percentage is 99.9999999999%)
how its possible
upload every file will be save in 3 az's (>3) will see on while file uploading.(apart from 1zoneIA) everything will be store on 3 az's)

will see on duration also.
**** max storage is s3 one file is 5tb.

s3 billing (file can access 3types of bills it considered).


*** s3 propertoes::
versioning:(it's not a complete if versioning show also if delete the data it will lost)
if unfortunately file will deleted its like small backup type.(we can overwrite or delete)
without versioning if u delete a file we can't retrive.

static website hosting:(
ur bucket will convert as a static website.

choose ur index document
unblock public access.
choose all files and make public

if u want like domain name.
note ******** if u use domain name u must ur bucket name also same.
cretate cucket
rajesh.com
use static web hosting (upload all files ) make it public
choose ur index document + save it
note :make it all public.
route53:
create a record set name: rajesh.com
type : a-ipv4 address
alias target (check it)
choose ur s3 url name save it.

if u want www also
crate a bucket.
www.rajesh.com
make public
under management tab + choose properties + static website + check on redirect request + under target bucket or domain name give ur exist bucket/domain name.
and http type + save
route53:
create a record set name: www.rajesh.com
type : a-ipv4 address
alias target (check it)
choose ur s3 url name save it.

note : it will used for simple web application.

under properties will see server access log.
enable
create a bucket to store logs (by default it won't store logs to store logs)+ permission + access control list + s3 log delievery (chech list + white + save).

choose bucket of logs
target prefix choose rajesh/ + save

every log file it will store but it will charge.

object level logging also u enable

*** cloud trail :
who are using ur aws acoount it will give u the total information.

encryption:
aes-256: it will encrypt the data.
aws-kms - aws will give the key to protect data

object lock:(u must do on creation time of bucket) immutable
only allow permission for file uploading not for deleting files

under advanced of creating bucket:
enable versioning.
enable object lock.

transfer accelartion:
while uploading u want more speed of file uploading.
they will provide one api url;
we are using in our application.

Events:

to perform some operation we are using events,

requester pays:

who are going to download they need to pay the money to aws.

permissions & management:

by default block all public access will enable(most of time we don;t give public access)
accees control list (old no one are using)

bucket polocy:
we can allow specific ip.

{
  "Version": "2012-10-17",
  "Id": "S3PolicyId1",
  "Statement": [
    {
      "Sid": "IPAllow",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:*",
      "Resource": [
	       "arn:aws:s3:::DOC-EXAMPLE-BUCKET",
         "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"
      ],
      "Condition": {
	 "NotIpAddress": {"aws:SourceIp": "54.240.143.0/24"}
      }
    }
  ]
}

ref url : https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html

polocy generator:
https://awspolicygen.s3.amazonaws.com/policygen.html


need to learn bucket polocies for interview process
need to practise more aws iam and role in polocies

by using iam bucket polocy we can manage


CORS:
CROSS ORIGIN RESOURCE SHARING.
different application domain names access to ur bucket
we don't use in real time rarely


MANAGEMENT:


LIFECLYCLE: old data we can transistion from one position to another position
add lifeclycle + name + apply to all objects + next + current version + object creation (standard 1a after 30 days) + after transistion to glacier 60 days+ next expire
3650 days + next + save
now it will create one life cycle

it won't revert back to stage.
if its go to glacier it will dificult to retrive data.

REPLICATION:

data will shift from across multiple regions (same region also)
inside bucket
add versioning + same account or diff account + choose one zone + next + create a new iam role + next + save.

if u upload any file into one bucket it will be also move to another region bucket.

for log files will use replication.

metrics:
who are access the files

Inventory:
what type of files are creating it will give report monthy or weekly


ACCESS POINTS:
ITS A NEW SERVICE DO R&D

from command line if u want to delete multiple buckets 
aws s3 rm s3://bucket name


s3 GLACIER:
create a vault.if u want fast retrive it chargable 
chep storage
s3 pricing

it contain very old data
third party tool
fast glacier is gacier tool (uplosading is easy retrie lot of time)



*** EFS
elastic file system

ebs dis advantage
1 ebs volume is used for only one server
2 ebs volume will applicable for same az of diff server

Storage types
Storage are 2 types 1.ebs 2. Instance store 3. Elastic file system

In ebs we need to format the disk and create file system on it.
But in efs : file store - acess the network ans then mount it and save the files .no need to formating creating a file system

Other storages are 
S3
Glacier

Ebs features:
Persistance or permanent storage
Ebs volumes will be creased but not decreased
We can have the volume between 1gb to 16tb

Handson : deploy a machine → (note if u want instance store choose m3.medium with 1*4 ssd )-->under add add storage (root size is 8gb ) create one ebs volume with 3gb → launch the instance.
Go to terminal:
sudo su && cd
lsblk (here we can see that all our storage information and it is mounted or not we can see that)
Note: after creating a instance also we can create a volume and we can attach it also.

But we can’t use it bcoz we format it and we make file system on it and we mountit later only we can use in ebs volumes.
Now i am going  to create a directory with newebsvolume mkdir newebsvolume
First i need to format the newly creted volume for this 
fdisk /dev/xvdf (here xvdf is the name of newly created volume we can find using by lsblk command)
m after n (for new partition ) after press p after enter 3 times note when u see command u need to type w
Now if we can see lsblk now our new volume will be created a new partition.
As of now we converted our raw volume to partition volume for further usuage
Now we need to create a file system → mkfs.ext3 /dev/xvdf1 (xvdf1 is the name of partition volume)+ente
Now we are going to mount the volume.
mount /dev/xvdf1 /newebsvolume/ (here newebsvolume it is our newly created directory) + enter
Now type df -h (here u can see the space of your instance)
Now we have to configure our partion volumes in fstab (for permanet storage on restart server no loss info
vi /etc/fstab (in that file write below code)
/dev/xvdf1  /newebsvolume         ext3   defaults   0    0  (save the file)
Go to newebsvolume crete some files 
Stop and start your server or instance loginto your  console your data will be there)
***** note: if u r not mention ur partition details on fstab on start the server ur data will be their but it won’t show in under the command of df -h
Go to ec2 -> volumes → edit volumes → (u can increae the volume size but u can’t decrease)
Note if  u add extra size to your volumes u have to follow below procedure 



Type growpart /dev/xvdf 1
→ lsblk (check ur we can see increase partion)
→ but df -h (still showing previous volume we need to change it)
→ resize2fs /dev/xvdf1
→ now check df -h (the size will be increased).

---------------------------------------------------------------------------------------------------------------------------------------------------------------
Different types of volumes:
Create a instance → under add storage we can see that volume types (general purpose ssd gp2 default root one ) → add tags → launch the instance.

After instance created now i am going to create a volume
Ec2 → volumes → create a volume 
Gp2 → 1gb is min and 16tb is the maximum

** note iops is * 3 times os size of volume ex: size is 200 then iops is 600 it’ s for gp2
For gp2: min iops: 100
                Max: 10000
For io1: min 100
               Max: 32000
Iops: means input output per second
** but if u choose volume type ops ssd (io1) we can increase iops not depeding on size of volume 



ec2→ create a image → save in snapshot → launch one instance → using snapshot → go to terminal now whatever we had a packages in previous volumes that all will be present in newly created instance.

Efs : elastic file system (it's a multi availability zone)
It is network based
We need to pay how much we are using efs.

Aws console → type efs choose it ---> give a name and launch it.
After go to ec2 -> launch a normal 2 micro instance with security groups nfs with sssh 0.0.0., → go to instance console → now we have to mount efs file system to our instances.
refurL: https://docs.aws.amazon.com/efs/latest/ug/mounting-fs.html
sudo yum install -y amazon-efs-utils
mkdir efs
sudo mount -t efs -o tls fs-12345678:/ /mnt/efs

Cd efs
Create a file and sudo touch sample.txt


Its a shared file system as that both are in same network as we created a file in one instance it will be reflect to other instance tooooo.


----------- by madhu efs starts--------
efs create a file system + choose vpc + choose region + 
create a file system



















































































































































